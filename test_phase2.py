#!/usr/bin/env python3
"""Script de test pour la Phase 2 - Scraping Engine."""

import asyncio
import sys
from pathlib import Path

# Ajouter le r√©pertoire racine au path
sys.path.append(str(Path(__file__).parent))

try:
    from app.config import settings
    from app.database import AsyncSessionLocal
    from app.services.dataforseo_client import DataForSEOClient
    from app.services.url_deduplication import URLDeduplicationService
    from app.services.competitor_detection import CompetitorDetectionService
    from app.services.scraping_service import ScrapingService
    from app.models import Project, Keyword, Competitor
    print("‚úÖ Imports Phase 2 r√©ussis")
except ImportError as e:
    print(f"‚ùå Erreur d'import Phase 2: {e}")
    sys.exit(1)


async def test_dataforseo_client():
    """Test du client DataForSEO."""
    print("\nüîç Test du client DataForSEO...")
    
    try:
        client = DataForSEOClient()
        print("‚úÖ Client DataForSEO initialis√©")
        
        # Test de connexion (sans vraie API key)
        try:
            connection_test = await client.test_connection()
            if connection_test.get("connected"):
                print("‚úÖ Connexion DataForSEO r√©ussie")
                print(f"   Balance: {connection_test.get('user_info', {}).get('money', {}).get('balance', 'N/A')}")
            else:
                print("‚ö†Ô∏è  Connexion DataForSEO √©chou√©e (normal sans API key)")
                print(f"   Erreur: {connection_test.get('error', 'Inconnue')}")
        except Exception as e:
            print(f"‚ö†Ô∏è  Test de connexion √©chou√© (normal sans API key): {type(e).__name__}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur client DataForSEO: {e}")
        return False


async def test_url_deduplication():
    """Test du service de d√©duplication d'URLs."""
    print("\nüîç Test du service de d√©duplication d'URLs...")
    
    try:
        async with AsyncSessionLocal() as session:
            url_service = URLDeduplicationService(session)
            
            # Test de normalisation d'URL
            test_urls = [
                "https://www.example.com/product?utm_source=google&id=123",
                "http://example.com/product/?id=123&ref=test",
                "https://example.com/product?id=123",
            ]
            
            normalized_urls = []
            for url in test_urls:
                normalized = url_service.normalize_url(url)
                normalized_urls.append(normalized)
                print(f"   {url} -> {normalized}")
            
            # V√©rifier que les URLs similaires sont bien normalis√©es
            if len(set(normalized_urls)) < len(test_urls):
                print("‚úÖ Normalisation d'URLs fonctionne")
            else:
                print("‚ö†Ô∏è  Normalisation d'URLs pourrait √™tre am√©lior√©e")
            
            # Test d'extraction de domaine
            domain = url_service.extract_domain("https://www.amazon.fr/product/123")
            if domain == "amazon.fr":
                print("‚úÖ Extraction de domaine fonctionne")
            else:
                print(f"‚ö†Ô∏è  Extraction de domaine inattendue: {domain}")
            
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur service d√©duplication: {e}")
        return False


async def test_competitor_detection():
    """Test du service de d√©tection de concurrents."""
    print("\nüîç Test du service de d√©tection de concurrents...")
    
    try:
        async with AsyncSessionLocal() as session:
            competitor_service = CompetitorDetectionService(session)
            
            # Test d'extraction de domaine
            domain = competitor_service.extract_domain_from_url("https://www.amazon.fr/product/123")
            if domain == "amazon.fr":
                print("‚úÖ Extraction de domaine concurrent fonctionne")
            else:
                print(f"‚ö†Ô∏è  Extraction de domaine inattendue: {domain}")
            
            # Test de d√©tection de marketplace
            is_marketplace = competitor_service.is_marketplace_domain("amazon.fr")
            if is_marketplace:
                print("‚úÖ D√©tection de marketplace fonctionne")
            else:
                print("‚ö†Ô∏è  D√©tection de marketplace √©chou√©e")
            
            # Test de calcul de score d'autorit√©
            score = competitor_service.calculate_domain_authority_score(
                domain="amazon.fr",
                appearances=10,
                avg_position=2.5,
                price_competitiveness=0.7
            )
            if score > 50:
                print(f"‚úÖ Calcul de score d'autorit√© fonctionne: {score}")
            else:
                print(f"‚ö†Ô∏è  Score d'autorit√© faible: {score}")
            
            # Test de suggestion de nom
            suggested_name = competitor_service.suggest_competitor_name(
                "amazon.fr", 
                {"Amazon", "Amazon.fr"}
            )
            if suggested_name:
                print(f"‚úÖ Suggestion de nom fonctionne: {suggested_name}")
            else:
                print("‚ö†Ô∏è  Suggestion de nom √©chou√©e")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur service d√©tection concurrents: {e}")
        return False


async def test_scraping_service():
    """Test du service principal de scraping."""
    print("\nüîç Test du service principal de scraping...")
    
    try:
        async with AsyncSessionLocal() as session:
            scraping_service = ScrapingService(session)
            
            # Test d'extraction de donn√©es produit
            test_serp_item = {
                "title": "Test Product",
                "description": "Test description",
                "price": {"current": 29.99, "currency": "EUR", "regular": 39.99},
                "merchant": {"name": "Test Store", "url": "https://teststore.com"},
                "rating": {"rating_value": 4.5, "reviews_count": 123},
                "url": "https://teststore.com/product/123"
            }
            
            product_data = scraping_service.extract_product_data(test_serp_item)
            
            if (product_data.get("title") == "Test Product" and 
                product_data.get("price") == 29.99 and
                product_data.get("merchant_name") == "Test Store"):
                print("‚úÖ Extraction de donn√©es produit fonctionne")
            else:
                print("‚ö†Ô∏è  Extraction de donn√©es produit incompl√®te")
                print(f"   Donn√©es extraites: {product_data}")
            
            # Test d'extraction de domaine
            domain = scraping_service.extract_domain_from_url("https://www.example.com/test")
            if domain == "example.com":
                print("‚úÖ Extraction de domaine scraping fonctionne")
            else:
                print(f"‚ö†Ô∏è  Extraction de domaine inattendue: {domain}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur service scraping: {e}")
        return False


def test_configuration_phase2():
    """Test de configuration sp√©cifique √† la Phase 2."""
    print("\nüîç Test de configuration Phase 2...")
    
    # V√©rifier les settings DataForSEO
    checks = [
        ("DATAFORSEO_BASE_URL", settings.dataforseo_base_url),
        ("MAX_CONCURRENT_REQUESTS", settings.max_concurrent_requests),
        ("REQUEST_DELAY_SECONDS", settings.request_delay_seconds),
        ("MAX_RETRIES", settings.max_retries),
    ]
    
    all_good = True
    for name, value in checks:
        if value:
            print(f"‚úÖ {name}: {value}")
        else:
            print(f"‚ö†Ô∏è  {name}: non configur√©")
            if name in ["DATAFORSEO_BASE_URL"]:
                all_good = False
    
    # V√©rifier les identifiants DataForSEO (sans les afficher)
    if settings.dataforseo_login and settings.dataforseo_password:
        print("‚úÖ Identifiants DataForSEO configur√©s")
    else:
        print("‚ö†Ô∏è  Identifiants DataForSEO manquants (normal pour les tests)")
    
    return all_good


async def test_database_models():
    """Test des mod√®les de base de donn√©es."""
    print("\nüîç Test des mod√®les de base de donn√©es...")
    
    try:
        async with AsyncSessionLocal() as session:
            # Test de cr√©ation d'un projet
            project = Project(
                name="Test Project Phase 2",
                description="Projet de test pour la phase 2"
            )
            session.add(project)
            await session.flush()
            
            # Test de cr√©ation d'un concurrent
            competitor = Competitor(
                project_id=project.id,
                name="Test Competitor",
                domain="testcompetitor.com",
                brand_name="Test Brand"
            )
            session.add(competitor)
            await session.flush()
            
            # Test de cr√©ation d'un mot-cl√©
            keyword = Keyword(
                project_id=project.id,
                keyword="test keyword phase 2",
                location="France",
                language="fr"
            )
            session.add(keyword)
            await session.flush()
            
            print("‚úÖ Mod√®les de base de donn√©es fonctionnent")
            
            # Rollback pour ne pas polluer la DB
            await session.rollback()
            
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur mod√®les de base de donn√©es: {e}")
        return False


async def main():
    """Fonction principale de test Phase 2."""
    print("üöÄ Test de la Phase 2 - Scraping Engine\n")
    
    # Tests
    config_ok = test_configuration_phase2()
    models_ok = await test_database_models()
    dataforseo_ok = await test_dataforseo_client()
    url_dedup_ok = await test_url_deduplication()
    competitor_ok = await test_competitor_detection()
    scraping_ok = await test_scraping_service()
    
    print("\n" + "="*60)
    print("üìä R√âSULTATS DES TESTS PHASE 2")
    print("="*60)
    
    results = [
        ("Configuration Phase 2", config_ok),
        ("Mod√®les DB", models_ok),
        ("Client DataForSEO", dataforseo_ok),
        ("D√©duplication URLs", url_dedup_ok),
        ("D√©tection concurrents", competitor_ok),
        ("Service scraping", scraping_ok),
    ]
    
    all_passed = True
    for test_name, passed in results:
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"{test_name:25} : {status}")
        if not passed:
            all_passed = False
    
    print("\n" + "="*60)
    if all_passed:
        print("üéâ Tous les tests Phase 2 sont pass√©s !")
        print("\nLa Phase 2 - Scraping Engine est pr√™te !")
        print("\nPour tester l'API :")
        print("  1. D√©marrer l'application : uvicorn app.main:app --reload")
        print("  2. Aller sur : http://localhost:8000/docs")
        print("  3. Tester les endpoints /api/v1/projects et /api/v1/scraping")
        print("\nEndpoints disponibles :")
        print("  - POST /api/v1/projects (cr√©er un projet)")
        print("  - GET  /api/v1/projects (lister les projets)")
        print("  - POST /api/v1/projects/{id}/scrape (scraper un projet)")
        print("  - GET  /api/v1/scraping/status/{task_id} (statut scraping)")
        print("  - POST /api/v1/test-dataforseo-connection (test DataForSEO)")
    else:
        print("‚ö†Ô∏è  Certains tests Phase 2 ont √©chou√©.")
        print("\nActions recommand√©es :")
        if not config_ok:
            print("  - V√©rifiez la configuration dans .env")
        if not models_ok:
            print("  - V√©rifiez la connexion √† la base de donn√©es")
        if not dataforseo_ok:
            print("  - Configurez les identifiants DataForSEO si n√©cessaire")
    
    return 0 if all_passed else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 